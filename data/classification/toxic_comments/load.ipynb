{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "c2ea6ed944d6425ee8782cf654adb8ea89a16b622c320e18cab62be437316a17"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Text Data Processing with PyTorch\n",
    "\n",
    "[Original tutorial](https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-i-5da6f1c89d84)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import (\n",
    "    Dataset\n",
    "    , DataLoader\n",
    "    , random_split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a small subset of the full toxic comment dataset\n",
    "df_sample = pd.read_csv(\"./sample.csv\", index_col=\"id\")\n",
    "df_sample.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self, freq_bar=5):\n",
    "\n",
    "        self.is_built = False\n",
    "        self.freq_bar = freq_bar\n",
    "        self.tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "        self.itos = {\n",
    "            0: \"<PAD>\"    # Padding\n",
    "            , 1: \"<SOS>\"  # Start of Sentence\n",
    "            , 2: \"<EOS>\"  # End of Sentence\n",
    "            , 3: \"<UNK>\"  # Unknown\n",
    "        }\n",
    "        self.stoi = {\n",
    "            \"<PAD>\": 0\n",
    "            , \"<SOS>\": 1\n",
    "            , \"<EOS>\": 2\n",
    "            , \"<UNK>\": 3\n",
    "        }\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def tokenize(self, text, max_num_char=2000):\n",
    "\n",
    "        # Chop off long string and tokenize\n",
    "        text = str(text)[:max_num_char]\n",
    "\n",
    "        # Preprocess special symbols before tokenization\n",
    "        text = re.sub(r\"[\\*\\\"“”\\r\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", text)\n",
    "        text = re.sub(r\"[ ]+\", \" \", text)\n",
    "        text = re.sub(r\"\\!+\", \"!\", text)\n",
    "        text = re.sub(r\"\\,+\", \",\", text)\n",
    "        text = re.sub(r\"\\?+\", \"?\", text)\n",
    "\n",
    "        # Use spaCy tokenizer\n",
    "        ret = [t.text for t in self.tokenizer(text) if t.text != \" \"]\n",
    "        return ret\n",
    "\n",
    "    def build_vocab(self, list_texts):\n",
    "\n",
    "        freq_stat, idx = {}, 4  # idx 3 is <UNK>\n",
    "        for text in list_texts:\n",
    "            for token in self.tokenize(text):\n",
    "\n",
    "                if (token not in freq_stat):\n",
    "                    freq_stat[token] = 1\n",
    "                else:\n",
    "                    freq_stat[token] += 1\n",
    "\n",
    "                if (freq_stat[token] == self.freq_bar):\n",
    "                    self.stoi[token] = idx\n",
    "                    self.itos[idx] = token\n",
    "                    idx += 1\n",
    "        self.is_built = True\n",
    "        return\n",
    "\n",
    "    def text_to_num(self, text):\n",
    "\n",
    "        if (not self.is_built):\n",
    "            raise(Exception(\"[ ERROR ] :: Vocabulary not built\"))\n",
    "        try:\n",
    "            ret = [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in  self.tokenize(text)]\n",
    "        except:\n",
    "            print(f\"[ ERROR ] :: Tokenization failed for < {text} >\")\n",
    "        return [self.stoi[\"<SOS>\"]] + ret + [self.stoi[\"<EOS>\"]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicComtDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "\n",
    "        # Index:\n",
    "        #   \"id\"\n",
    "        # Predictor/Factor:\n",
    "        #   \"comment_text\"\n",
    "        # Labels/Targets:\n",
    "        #   \"toxic\"\n",
    "        #   \"severe_toxic\"\n",
    "        #   \"obscene\"\n",
    "        #   \"threat\"\n",
    "        #   \"insult\"\n",
    "        #   \"identity_hate\"\n",
    "        self.df = pd.read_csv(path, index_col=\"id\")\n",
    "        self.factor_raw = self.df[\"comment_text\"]\n",
    "\n",
    "        # Build a vocabulary\n",
    "        self.vocab = Vocabulary()\n",
    "        self.vocab.build_vocab(self.factor_raw)\n",
    "\n",
    "        # A temporary transformer for numericalize comments\n",
    "        def text_to_tensor(text):\n",
    "\n",
    "            ret = self.vocab.text_to_num(text)\n",
    "            ret = torch.tensor(ret, dtype=torch.int32)\n",
    "            return ret\n",
    "\n",
    "        # Apply to all comments\n",
    "        self.factor = self.factor_raw.apply(text_to_tensor)\n",
    "\n",
    "        # Select just one target for binary classification\n",
    "        target_col = \"toxic\"\n",
    "        self.target = torch.tensor(\n",
    "            self.df.loc[:, target_col].values, dtype=torch.uint8\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.factor[idx], self.target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicComtDataLoader(object):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        return"
   ]
  },
  {
   "source": [
    "## Load Tabular Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series([[1, 2, 3], [2, 1, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ToxicComtDataset(\"sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo = DataLoader(ds, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<SOS> Yes , it looks much better than before . At the top of the page , it says This article needs additional citations for <UNK> , may be <UNK> encyclopedia could help adding citations ... I <UNK> . Cheers , <EOS> 214 tensor([0], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "_, (x, y) = next(enumerate(lo))\n",
    "s = ' '.join([ds.vocab.itos[int(i)] for i in x[0]])\n",
    "print(s, len(s), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 266, 105, 7, 6, 3, 79, 22, 139, 8, 176, 2]"
      ]
     },
     "metadata": {},
     "execution_count": 170
    }
   ],
   "source": [
    "ds.vocab.text_to_num(\"Hi there, I wonder what is going on here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}