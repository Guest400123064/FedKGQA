{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('th-federated': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c2ea6ed944d6425ee8782cf654adb8ea89a16b622c320e18cab62be437316a17"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Text Data Processing with PyTorch\n",
    "\n",
    "[Original tutorial](https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-i-5da6f1c89d84)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import (\n",
    "    Dataset\n",
    "    , DataLoader\n",
    "    , random_split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0000997932d777bf  Explanation\\r\\nWhy the edits made under my use...      0   \n",
       "000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "0001b41b1c6bb37e  \"\\r\\nMore\\r\\nI can't make any real suggestions...      0   \n",
       "0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "id                                                                      \n",
       "0000997932d777bf             0        0       0       0              0  \n",
       "000103f0d9cfb60f             0        0       0       0              0  \n",
       "000113f07ec002fd             0        0       0       0              0  \n",
       "0001b41b1c6bb37e             0        0       0       0              0  \n",
       "0001d958c54c6e35             0        0       0       0              0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0000997932d777bf</th>\n      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>000103f0d9cfb60f</th>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>000113f07ec002fd</th>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0001b41b1c6bb37e</th>\n      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0001d958c54c6e35</th>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# This is a small subset of the full toxic comment dataset\n",
    "df_sample = pd.read_csv(\"./sample.csv\", index_col=\"id\")\n",
    "df_sample.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self, freq_bar=5):\n",
    "\n",
    "        self.is_built = False\n",
    "        self.freq_bar = freq_bar\n",
    "        self.tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "        self.itos = {\n",
    "            0: \"<PAD>\"    # Padding\n",
    "            , 1: \"<SOS>\"  # Start of Sentence\n",
    "            , 2: \"<EOS>\"  # End of Sentence\n",
    "            , 3: \"<UNK>\"  # Unknown\n",
    "        }\n",
    "        self.stoi = {\n",
    "            \"<PAD>\": 0\n",
    "            , \"<SOS>\": 1\n",
    "            , \"<EOS>\": 2\n",
    "            , \"<UNK>\": 3\n",
    "        }\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def tokenize(self, text, max_num_char=2000):\n",
    "\n",
    "        # Chop off long string and tokenize\n",
    "        text = str(text)[:max_num_char]\n",
    "\n",
    "        # Preprocess special symbols before tokenization\n",
    "        text = re.sub(r\"[\\*\\\"“”\\r\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", text)\n",
    "        text = re.sub(r\"[ ]+\", \" \", text)\n",
    "        text = re.sub(r\"\\!+\", \"!\", text)\n",
    "        text = re.sub(r\"\\,+\", \",\", text)\n",
    "        text = re.sub(r\"\\?+\", \"?\", text)\n",
    "\n",
    "        # Use spaCy tokenizer\n",
    "        ret = [t.text for t in self.tokenizer(text) if t.text != \" \"]\n",
    "        return ret\n",
    "\n",
    "    def build_vocab(self, list_texts):\n",
    "\n",
    "        freq_stat, idx = {}, 4  # start from idx4; idx 3 is <UNK>\n",
    "        for text in list_texts:\n",
    "            for token in self.tokenize(text):\n",
    "\n",
    "                if (token not in freq_stat):\n",
    "                    freq_stat[token] = 1\n",
    "                else:\n",
    "                    freq_stat[token] += 1\n",
    "\n",
    "                if (freq_stat[token] == self.freq_bar):\n",
    "                    self.stoi[token] = idx\n",
    "                    self.itos[idx] = token\n",
    "                    idx += 1\n",
    "        self.is_built = True\n",
    "        return\n",
    "\n",
    "    def text_to_num(self, text):\n",
    "\n",
    "        if (not self.is_built):\n",
    "            raise(Exception(\"[ ERROR ] :: Vocabulary not built\"))\n",
    "        try:\n",
    "            ret = [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in  self.tokenize(text)]\n",
    "        except:\n",
    "            print(f\"[ ERROR ] :: Tokenization failed for < {text} >\")\n",
    "        return [self.stoi[\"<SOS>\"]] + ret + [self.stoi[\"<EOS>\"]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicComtDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "\n",
    "        # Index:\n",
    "        #   \"id\"\n",
    "        # Predictor/Factor:\n",
    "        #   \"comment_text\"\n",
    "        # Labels/Targets:\n",
    "        #   \"toxic\"\n",
    "        #   \"severe_toxic\"\n",
    "        #   \"obscene\"\n",
    "        #   \"threat\"\n",
    "        #   \"insult\"\n",
    "        #   \"identity_hate\"\n",
    "        self.df = pd.read_csv(path, index_col=\"id\")\n",
    "        self.factor_raw = self.df[\"comment_text\"]\n",
    "\n",
    "        # Build a vocabulary\n",
    "        self.vocab = Vocabulary()\n",
    "        self.vocab.build_vocab(self.factor_raw)\n",
    "\n",
    "        # A temporary transformer for numericalize comments\n",
    "        def text_to_tensor(text):\n",
    "\n",
    "            ret = self.vocab.text_to_num(text)\n",
    "            ret = torch.tensor(ret, dtype=torch.long)\n",
    "            return ret\n",
    "\n",
    "        # Apply to all comments\n",
    "        self.factor = self.factor_raw.apply(text_to_tensor)\n",
    "\n",
    "        # Select just one target for binary classification\n",
    "        target_col = \"toxic\"\n",
    "        self.target = torch.tensor(\n",
    "            self.df.loc[:, target_col].values, dtype=torch.uint8\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.factor[idx], self.target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicComtCollate(object):\n",
    "\n",
    "    def __init__(self, idx_pad):\n",
    "        \n",
    "        self.idx_pad = idx_pad\n",
    "        return\n",
    "\n",
    "    def __call__(self, batch):\n",
    "\n",
    "        # The following command equates:\n",
    "        #   factors = [t[0] for t in batch]\n",
    "        #   targets = [t[1] for t in batch]\n",
    "        factors, targets = zip(*batch)\n",
    "        factors = pad_sequence(\n",
    "            factors\n",
    "            , batch_first=True\n",
    "            , padding_value=self.idx_pad\n",
    "        )\n",
    "        return factors, targets\n",
    "\n",
    "\n",
    "class ToxicComtDataLoader(object):\n",
    "\n",
    "    def __init__(\n",
    "        self\n",
    "        , path_root\n",
    "        , path_file\n",
    "        , prob_train=0.7\n",
    "    ):\n",
    "\n",
    "        # Get data\n",
    "        path_full = os.path.join(path_root, path_file)\n",
    "        self.dataset = ToxicComtDataset(path_full)\n",
    "\n",
    "        # Random split\n",
    "        len_train = int(prob_train * len(self.dataset))\n",
    "        len_valid = len(self.dataset) - len_train\n",
    "        dataset_train, dataset_valid = random_split(\n",
    "            self.dataset, [len_train, len_valid]\n",
    "        )\n",
    "\n",
    "        # Make loader\n",
    "        collate_fn = ToxicComtCollate(self.dataset.vocab.stoi.get(\"<PAD>\"))\n",
    "        self.loader_train = DataLoader(\n",
    "            dataset_train\n",
    "            , batch_size=4\n",
    "            , shuffle=True\n",
    "            , collate_fn=collate_fn\n",
    "        )\n",
    "        self.loader_valid = DataLoader(\n",
    "            dataset_valid\n",
    "            , batch_size=4\n",
    "            , shuffle=True\n",
    "            , collate_fn=collate_fn\n",
    "        )\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_loader = ToxicComtDataLoader(\".\", \"sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (x, y) = next(enumerate(toxic_loader.loader_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[   1,  981,    5,  712,  957,   43,    3,    7,  892,   16,   51,   28,\n",
       "         1220,    3,  141,    3,   29, 1922,  745,   12,  847,    9,   35,   62,\n",
       "            4,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    3,    3,    6,  169, 1762,  117,   57, 1040,   50,  220,    4,\n",
       "          537,    7,    3,    4,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1, 1327,   26,    5,  564,    8,   64,    3,   22,    9, 1078,  639,\n",
       "            3,   29,  957,    3,  125,    5,    3,   29,  464,    3,   20,    5,\n",
       "         1021, 1920,    4,  137,   12,   33,   17,  671,    4, 1778,  762,   16,\n",
       "          169,  186,   48, 1920,   16,  131,    9,  457,    5,  487,   20,   15,\n",
       "            7,   17, 1534,   20,    5,    3,    3,    3,    5,  383,    9,    5,\n",
       "            3,   22, 1431,   98, 1779,   15,    4,  166,  725,  175,    7,   50,\n",
       "            6,  155,    3,    5,    3,  295,    4, 1498,    7,   79,    6,   10,\n",
       "           22,  380,    5,  835,  218,   28, 1744,    4,  179,    2],\n",
       "        [   1,    3,   21,    3,   44,    3,    3,    7,    3,    3, 1929,    4,\n",
       "            2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.nn.Embedding(len(toxic_loader.dataset.vocab), 3)\n",
    "lstm = torch.nn.LSTM(input_size=3, hidden_size=5, batch_first=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = embedding(x)\n",
    "o, (h, c) = lstm(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 94, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "o.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}