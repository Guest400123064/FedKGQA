{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('th-federated': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c2ea6ed944d6425ee8782cf654adb8ea89a16b622c320e18cab62be437316a17"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import (\n",
    "    DataLoader\n",
    "    , Dataset\n",
    "    , RandomSampler\n",
    "    , random_split\n",
    ")"
   ]
  },
  {
   "source": [
    "# Modularity in Deep Learning Projects\n",
    "\n",
    "To make our code more understandable, we'd better decompose our huge blobs of modeling program into a collection smaller and easy-to-read code files. Further, we can organize these smaller pieces of code files into several categories, namely: \n",
    "\n",
    "* Data Loader\n",
    "* Graphs\n",
    "    + Models\n",
    "    + Loss Layers\n",
    "* Utils\n",
    "* Config\n",
    "\n",
    "To piece these seperate components together, we use the so-called **Agents**. In plain English or Computer Science Terminologies: main function (actually the main \"object\" instead of a function, explained later). To conduct experiments, we simply instantiate the corresponding Agent in a driver function and call the corresponding methods of our Agent.\n",
    "\n",
    "In short, we are able to sort out the dependencies between code files in a hiearchical mannar that reduces the difficulties for others to approach our project. This is **NOT** my novel idea. The project template I deem really satisfying can be found [here](https://github.com/moemen95/PyTorch-Project-Template#tutorials) (please give them a star :D). Their GitHub Repo already contains everything that thoroughly explain the idea mentioned above. However, the code files are scattered in their own folders, making referencing between files annoying. So, in this tutorial, I will try to give a vanilla version and put all of the modules in a single file (note that some of the features may get lost)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data Loader\n",
    "\n",
    "Generally speaking, the Data Loaders convert raw data files, no matter the fomat, into tensors for training. For `PyTorch` projects, we'd better learner to use `DataLoader` and `Dataset` abstractions to help us organize the datasets. A more detailed introduction regarding these two objects are introduced in another tutorial. Here I simply copy the code for `Breast Cancer` dataset loading from that tutorial.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastCancerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "\n",
    "        return"
   ]
  }
 ]
}