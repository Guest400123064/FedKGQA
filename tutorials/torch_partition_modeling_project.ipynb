{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('th-federated': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c2ea6ed944d6425ee8782cf654adb8ea89a16b622c320e18cab62be437316a17"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from easydict import EasyDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import (\n",
    "    DataLoader\n",
    "    , Dataset\n",
    "    , SubsetRandomSampler\n",
    ")"
   ]
  },
  {
   "source": [
    "# Modularity in Deep Learning Projects\n",
    "\n",
    "To make our code more understandable, we'd better decompose our huge blobs of modeling program into a collection smaller and easy-to-read code files. Further, we can organize these smaller pieces of code files into several categories, namely: \n",
    "\n",
    "* Data Loader\n",
    "* Graphs\n",
    "    + Models\n",
    "    + Loss Layers\n",
    "* Utils\n",
    "* Config\n",
    "\n",
    "To piece these seperate components together, we use the so-called **Agents**. In plain English or Computer Science Terminologies: main function (actually the main \"object\" instead of a function, explained later). To conduct experiments, we simply instantiate the corresponding Agent in a driver function and call the corresponding methods of our Agent.\n",
    "\n",
    "In short, we are able to sort out the dependencies between code files in a hiearchical mannar that reduces the difficulties for others to approach our project. This is **NOT** my novel idea. The project template I deem really satisfying can be found [here](https://github.com/moemen95/PyTorch-Project-Template#tutorials) (please give them a star :D). \n",
    "\n",
    "Although their GitHub Repo already contains everything that thoroughly explain the idea mentioned above, the code files are scattered in their own folders, making referencing files back and forth annoying. In this tutorial, I will try to give a vanilla version and put all of the modules in a single file (note that some of the features may get lost). The modeling project in this notebook is a *Binary Classification* problem using *Logistic Regression* model based on the *Breast Cancer* dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data Loader\n",
    "\n",
    "Generally speaking, the Data Loaders convert raw data files, no matter the fomat, into tensors for training. For `PyTorch` projects, we'd better learner to use `DataLoader` and `Dataset` abstractions to help us organize the datasets. A more detailed introduction regarding these two objects are introduced in another tutorial. Here I simply copy the code for `Breast Cancer` dataset loading from that tutorial.\n",
    "\n",
    "In a word, we use `Dataset` to read in the raw data file and convert them into *a set of Records*, e.g. a pair of predictors and corresponding target stored in a tuple. And we define how we can slice the dataset. `DataLoader` are then created based on a certain dataset, and it gives us a uniform interface to access and operate the underlying dataset during the training session.\n",
    "\n",
    "So the general workflow is to instantiate a `DataLoader`, and, during the instantiation, it (1) creates a `Dataset` object, (2) shuffles the dataset, (3) \"splits\" the set according to certain configuration, and (4) defines iteration scheme, such as batch size. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastCancerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "\n",
    "        # Let's just use pandas to read csv data\n",
    "        self.df = pd.read_csv(\n",
    "            path\n",
    "            , header=None  # This file contains no header\n",
    "            , index_col=0  # First column is an index column \n",
    "        ).replace({\n",
    "\n",
    "            # Recode the targets such that:\n",
    "            #   M(alignant) == 1\n",
    "            #   B(enign) == 0\n",
    "            1: {'M': 1, 'B': 0}  # 1 denote the second column\n",
    "        })\n",
    "\n",
    "        # Split X, Y and convert to tensors\n",
    "        self.factor = torch.tensor(\n",
    "            self.df.iloc[:, 1:].values, dtype=torch.float\n",
    "        )\n",
    "        self.target = torch.tensor(\n",
    "            self.df.iloc[:, :1].values, dtype=torch.float\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # Return # target\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Return a tuple with the first element being the predictors\n",
    "        return self.factor[idx], self.target[idx]\n",
    "\n",
    "\n",
    "class BreastCancerDataLoader(object):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = EasyDict(config)\n",
    "\n",
    "        # Load data\n",
    "        self.dataset = BreastCancerDataset(\n",
    "            os.path.join(self.config.path_dir, self.config.path_file)\n",
    "        )\n",
    "\n",
    "        # Split train test, possibly dev set\n",
    "        #   1. Create indices\n",
    "        #   2. Make a sampler\n",
    "        #   3. Create seperate data loaders, feeding both the datset and sampler\n",
    "        n_sample = len(self.dataset)\n",
    "        cut_train = int(self.config.pct_train * n_sample)\n",
    "        idxs_full = np.arange(n_sample)[torch.randperm(n_sample)]  # Shuffle\n",
    "\n",
    "        self.idxs_train = idxs_full[:cut_train]\n",
    "        self.idxs_valid = idxs_full[cut_train:]\n",
    "\n",
    "        splr_train = SubsetRandomSampler(self.idxs_train)\n",
    "        splr_valid = SubsetRandomSampler(self.idxs_valid)\n",
    "\n",
    "        self.loader_train = DataLoader(\n",
    "            self.dataset\n",
    "            , sampler=splr_train\n",
    "            , batch_size=self.config.batch_size\n",
    "        )\n",
    "        self.loader_valid = DataLoader(\n",
    "            self.dataset\n",
    "            , sampler=splr_valid\n",
    "            , batch_size=self.config.batch_size\n",
    "        )\n",
    "        return"
   ]
  },
  {
   "source": [
    "## Graph: Model and Loss Function\n",
    "\n",
    "For this simple demo, the logistic regression, we don't really need a self-defined loss function. We simply instantiate the `BCELoss` object. Here we only define the logistic regression class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegres(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = EasyDict(config)\n",
    "\n",
    "        self.linear = nn.Linear(\n",
    "            self.config.n_factor\n",
    "            , out_features=1\n",
    "            , bias=True\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        linear = self.linear(x)\n",
    "        return torch.sigmoid(linear)"
   ]
  },
  {
   "source": [
    "## Combines Everything: Agent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastCancerAgent(object):\n",
    "\n",
    "    def __init__(self, config_data, config_model):\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_DATA = {\n",
    "    \"path_dir\": \"../data/classification/breast_cancer\"\n",
    "    , \"path_file\": \"data.csv\"\n",
    "    , \"pct_train\": 0.7\n",
    "    , \"batch_size\": 4\n",
    "}\n",
    "\n",
    "CONFIG_MODEL = {\n",
    "    \"n_factor\": 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}