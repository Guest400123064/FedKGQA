{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('th-federated': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c2ea6ed944d6425ee8782cf654adb8ea89a16b622c320e18cab62be437316a17"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from easydict import EasyDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import (\n",
    "    DataLoader\n",
    "    , Dataset\n",
    "    , SubsetRandomSampler\n",
    ")"
   ]
  },
  {
   "source": [
    "# Modularity in Deep Learning Projects\n",
    "\n",
    "To make our code more understandable, we'd better decompose our huge blobs of modeling program into a collection smaller and easy-to-read code files. Further, we can organize these smaller pieces of code files into several categories, namely: \n",
    "\n",
    "* Data Loader\n",
    "* Graphs\n",
    "    + Models\n",
    "    + Loss Layers\n",
    "* Utils\n",
    "* Config\n",
    "\n",
    "To piece these seperate components together, we use the so-called **Agents**. In plain English or Computer Science Terminologies: main function (actually the main \"object\" instead of a function, explained later). To conduct experiments, we simply instantiate the corresponding Agent in a driver function and call the corresponding methods of our Agent.\n",
    "\n",
    "In short, we are able to sort out the dependencies between code files in a hiearchical mannar that reduces the difficulties for others to approach our project. This is **NOT** my novel idea. The project template I deem really satisfying can be found [here](https://github.com/moemen95/PyTorch-Project-Template#tutorials) (please give them a star :D). \n",
    "\n",
    "Although their GitHub Repo already contains everything that thoroughly explain the idea mentioned above, the code files are scattered in their own folders, making referencing files back and forth annoying. In this tutorial, I will try to give a vanilla version and put all of the modules in a single file (note that some of the features may get lost). The modeling project in this notebook is a *Binary Classification* problem using *Logistic Regression* model based on the *Breast Cancer* dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data Loader\n",
    "\n",
    "Generally speaking, the Data Loaders convert raw data files, no matter the fomat, into tensors for training. For `PyTorch` projects, we'd better learner to use `DataLoader` and `Dataset` abstractions to help us organize the datasets. A more detailed introduction regarding these two objects are introduced in another tutorial. \n",
    "\n",
    "In a word, we use `Dataset` to read in the raw data file and convert them into *a set of Records*, e.g. a pair of predictors and corresponding target stored in a tuple. And we define how we can slice the dataset. `DataLoader` are then created based on a certain dataset, and it gives us a uniform interface to access and operate the underlying dataset during the training session.\n",
    "\n",
    "So the general workflow is to instantiate a `DataLoader`, and, during the instantiation, it (1) creates a `Dataset` object, (2) shuffles the dataset, (3) \"splits\" the set according to certain configuration, and (4) defines iteration scheme, such as batch size. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastCancerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "\n",
    "        # Let's just use pandas to read csv data\n",
    "        self.df = pd.read_csv(\n",
    "            path\n",
    "            , header=None  # This file contains no header\n",
    "            , index_col=0  # First column is an index column \n",
    "        ).replace({\n",
    "\n",
    "            # Recode the targets such that:\n",
    "            #   M(alignant) == 1\n",
    "            #   B(enign) == 0\n",
    "            1: {'M': 1, 'B': 0}  # 1 denote the second column\n",
    "        })\n",
    "\n",
    "        # Split X, Y and convert to tensors\n",
    "        self.factor = torch.tensor(\n",
    "            self.df.iloc[:, 1:].values, dtype=torch.float\n",
    "        )\n",
    "        self.target = torch.tensor(\n",
    "            self.df.iloc[:, :1].values, dtype=torch.float\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # Return # target\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Return a tuple with the first element being the predictors\n",
    "        return self.factor[idx], self.target[idx]\n",
    "\n",
    "\n",
    "class BreastCancerDataLoader(object):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = EasyDict(config)\n",
    "\n",
    "        # Load data\n",
    "        self.dataset = BreastCancerDataset(\n",
    "            os.path.join(self.config.path_dir, self.config.path_file)\n",
    "        )\n",
    "\n",
    "        # Split train test, possibly dev set\n",
    "        #   1. Create indices\n",
    "        #   2. Make samplers\n",
    "        #   3. Create seperate data loaders, feeding both the datset and sampler\n",
    "        n_sample = len(self.dataset)\n",
    "        cut_train = int(self.config.pct_train * n_sample)\n",
    "        idxs_full = np.arange(n_sample)[torch.randperm(n_sample)]  # Shuffle\n",
    "\n",
    "        self.idxs_train = idxs_full[:cut_train]\n",
    "        self.idxs_valid = idxs_full[cut_train:]\n",
    "\n",
    "        splr_train = SubsetRandomSampler(self.idxs_train)\n",
    "        splr_valid = SubsetRandomSampler(self.idxs_valid)\n",
    "\n",
    "        self.loader_train = DataLoader(\n",
    "            self.dataset\n",
    "            , sampler=splr_train\n",
    "            , batch_size=self.config.batch_size\n",
    "        )\n",
    "        self.loader_valid = DataLoader(\n",
    "            self.dataset\n",
    "            , sampler=splr_valid\n",
    "            , batch_size=self.config.batch_size\n",
    "        )\n",
    "        return"
   ]
  },
  {
   "source": [
    "## Graph: Model and Loss Function\n",
    "\n",
    "For this simple demo, the logistic regression, we don't really need a self-defined loss function. We simply instantiate the `BCELoss` object. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = EasyDict(config)\n",
    "\n",
    "        self.linear = nn.Linear(\n",
    "            self.config.n_factor\n",
    "            , out_features=1\n",
    "            , bias=True\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        linear = self.linear(x)\n",
    "        return torch.sigmoid(linear)\n",
    "\n",
    "\n",
    "class BCELoss(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.loss_fn = nn.BCELoss(reduce=\"mean\")  # <-- average batch loss\n",
    "        return\n",
    "\n",
    "    def __call__(self, pred, target):\n",
    "\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            Any object with `__call__` method implemented can\n",
    "              act like a function. In this case, our loss function \n",
    "              is called by: `fun = BCELoss; fun(...)`\n",
    "        \"\"\"\n",
    "\n",
    "        return self.loss_fn(pred, target)"
   ]
  },
  {
   "source": [
    "## Combines Everything: Agent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastCancerLRAgent(object):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        # Model Init\n",
    "        self.loader = BreastCancerDataLoader(self.config.data)\n",
    "        self.model = LogisticRegression(self.config.model)\n",
    "        self.loss_fn = BCELoss()\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            self.model.parameters()\n",
    "            , lr=self.config.train.lr\n",
    "        )\n",
    "\n",
    "        # Counter Init\n",
    "        self.curr_epoch = 0\n",
    "        self.curr_iter = 0\n",
    "        return\n",
    "\n",
    "    def load_checkpoint(self, path_check_pt):\n",
    "\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            Check point loader, very useful if training a HUGE model and \n",
    "              some exception happened. Not necessarily this implementation.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_checkpoint(self, path_check_pt=\"checkpoint.pth.tar\", is_best=0):\n",
    "\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            Same as stated above.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            The entry, like the `main(...)` function. In the main \n",
    "              function, we simply call this method with one line of code.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            self.train()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"[ INFO ] :: Keyboard Interruption, session ends\")\n",
    "        return\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            The main worker function that drive the training session.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            Helper function of `self.train(...)`. One epoch of training.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def validate(self):\n",
    "\n",
    "        \"\"\"\n",
    "        One cycle of model validation\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def finalize(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Finalizes all the operations of the 2 Main classes of the process, the operator and the data loader\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"data\": {\n",
    "        \"path_dir\": \"../data/classification/breast_cancer\"\n",
    "        , \"path_file\": \"data.csv\"\n",
    "        , \"pct_train\": 0.7\n",
    "        , \"batch_size\": 4\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"n_factor\": 30\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"max_epoch\": 20\n",
    "        , \"lr\": 0.05\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}