{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('th-federated': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c2ea6ed944d6425ee8782cf654adb8ea89a16b622c320e18cab62be437316a17"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tokenizing Sentences with [spaCy](https://spacy.io/usage/spacy-101) Package"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "source": [
    "## 0. Installation\n",
    "\n",
    "Before runing the example we need to download a pre-trained \"parser\" by running:\n",
    "\n",
    "`python -m spacy download en_core_web_sm`\n",
    "\n",
    "Assuming that you have installed the spacy package. Better use `pip install -U spacy`\n",
    "\n",
    "国内安装的话去baidu搜一下手动下载`en_core_web_{sm, md}-版本号.tar.gz`然后用`python -m pip install en_core_web_{sm, md}-版本号.tar.gz`命令安装。安装的时候要改成清华源。Windows的话过程非常恶心，需要管理员权限，最好去搜一下。我最后也不是很确定怎么安装成功的。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Quick Example\n",
    "\n",
    "Note that in the first example, `U.K.` is treated as a whole while `do` and `n't` are splited apart. So the tokenizer does more than simply seperate words by white spaces."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "One example:\n0th token is: Apple\n1th token is: is\n2th token is: looking\n3th token is: at\n4th token is: buying\n5th token is: U.K.\n6th token is: startup\n7th token is: for\n8th token is: $\n9th token is: 1\n10th token is: billion\n-------------------------------\nAnother example:\n0th token is: I\n1th token is: do\n2th token is: n't\n3th token is: wanna\n4th token is: go\n5th token is: \n\n6th token is: to\n7th token is: school\n8th token is: !\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "print(\"One example:\")\n",
    "for i, token in enumerate(doc):\n",
    "    print(f\"{i}th token is: {token}\")\n",
    "\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "print(\"Another example:\")\n",
    "doc = nlp(\"I don't wanna go to school!\")\n",
    "for i, token in enumerate(doc):\n",
    "    print(f\"{i}th token is: {token}\")"
   ]
  },
  {
   "source": [
    "## 2. Sentences Containing Special Symbols\n",
    "\n",
    "Not all of the symbols are so meaningful and sometimes unwanted. For instance, the 7th token is an extra space, and there are a series of extra ! at the end. Thus, we can use `re.sub` to remove certain symbols beforehand."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tokenization before processing:\n0th token is: Troy\n1th token is: `\n2th token is: is\n3th token is: `\n4th token is: a\n5th token is: ^\n6th token is: very\n7th token is:      \n \n8th token is: nice\n9th token is: place\n10th token is: ~\n11th token is: (\n12th token is: or\n13th token is: town\n14th token is: )\n15th token is: to\n16th token is: live\n17th token is: in\n18th token is: !\n19th token is: !\n20th token is: !\n21th token is: !\n---------------------------------\nTokenization after processing:\n0th token is: Troy\n1th token is: is\n2th token is: a\n3th token is: very\n4th token is: nice\n5th token is: place\n6th token is: or\n7th token is: town\n8th token is: to\n9th token is: live\n10th token is: in\n11th token is: !\n"
     ]
    }
   ],
   "source": [
    "# Original string\n",
    "str_raw = \"Troy `is` a ^ very      \\n nice place ~ (or town) to live in!!!!\"\n",
    "\n",
    "# Remove special characters like: ^ or ~\n",
    "str_pro = re.sub(\n",
    "    pattern=r\"[\\(\\)`~^]\"  # [] groups a set of chracters to be matched\n",
    "    , repl=\" \"            # Replace them with a single space\n",
    "    , string=str_raw\n",
    ")\n",
    "\n",
    "# Remove extra characters\n",
    "str_pro = re.sub(\"\\n\", \" \", str_pro)\n",
    "str_pro = re.sub(\"[ ]+\", \" \", str_pro)\n",
    "str_pro = re.sub(\"\\!+\", \"!\", str_pro)\n",
    "\n",
    "print(\"Tokenization before processing:\")\n",
    "for i, token in enumerate(nlp(str_raw)):\n",
    "    print(f\"{i}th token is: {token}\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "print(\"Tokenization after processing:\")\n",
    "for i, token in enumerate(nlp.tokenizer(str_pro)):\n",
    "    print(f\"{i}th token is: {token.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}