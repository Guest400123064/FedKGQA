{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('th-federated': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c2ea6ed944d6425ee8782cf654adb8ea89a16b622c320e18cab62be437316a17"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import (\n",
    "    Dataset\n",
    "    , IterableDataset\n",
    "    , DataLoader\n",
    "    , SubsetRandomSampler\n",
    "    , random_split\n",
    ")"
   ]
  },
  {
   "source": [
    "# Building a Data Pipeline\n",
    "\n",
    "To build a data pipline with `PyTorch`, there are three major components:\n",
    "\n",
    "* `Dataset`\n",
    "* `DataLoader`\n",
    "* `Sampler`\n",
    "\n",
    "In general, the workflow would be: (1) load raw dataset from disk/web to create a torch `Dataset` object, (2) determine a sampling scheme and instantiate `Sampler` object(s), and lastly (3) glue the `Dataset` and `Sampler`, together with other training parameters, to generate a `DataLoader`. During the model training, we load batches of data by iterating through the `DataLoader` we have created. Examples will be given below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## The Dataset Object\n",
    "\n",
    "The official documentation can be found [here](https://pytorch.org/docs/stable/data.html). There are two types of datasets: (1) **Map-style Dataset** and (2) **Iterable-style Dataset**. In a word, map-style datasets act like a **list** or **table** that we can index on. For instance, we can select the 7th sample. On the other hand, the iterable-style datasets work like iterators, meaning that we only define how we to **retrieve next sample/batch**. Both of these two types of datasets can be very useful. I will introduce the map-style dataset as it is more intuitive. I also decide to save the `Iterable-style Dataset` for another tutorial because this notebook would become too long otherwise."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Map-style Dataset\n",
    "\n",
    "To construct a custom map-style dataset, we must implement at least three methods:\n",
    "\n",
    "* `__init__(self, ...)` (obviously)\n",
    "* `__len__(self)`\n",
    "* `__getitem__(self, idx)`\n",
    "\n",
    "Just in case, the third method is actually called through the square bracket operator `[]`. For instance `my_dataset[0]` gives the first item (well, it depends on how you would implement the method, e.g. whether it supports ranged slicing).\n",
    "\n",
    "A simple template is shown below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, path):    # <-- My habit, not necessarily <path> only\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):  # <-- ONE ARGUMENT ONLY!!!\n",
    "        pass\n",
    "\n",
    "# Sample creation of a dataset\n",
    "try:\n",
    "    my_dataset = MyDataset(\"~/path/to/your/raw/data.csv\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "source": [
    "#### A Toy Example\n",
    "\n",
    "The toy example consists only 8 samples with two attributes: factor and target, both of which are numerical data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   factor   target\n",
       "0    10.0        0\n",
       "1     5.0        1\n",
       "2     2.5        1\n",
       "3    11.0        0\n",
       "4    15.0        0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>factor</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "df_toy = pd.read_csv(\"../data/classification/toy_example/data.csv\")\n",
    "df_toy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyMapDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        It simply fetch the raw csv file, stored as pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \n",
    "        self.df = pd.read_csv(path)\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):  \n",
    "\n",
    "        return self.df.iloc[idx, :]\n"
   ]
  },
  {
   "source": [
    "# Instantiate one toy dataset and see its feature\n",
    "toy_map_dataset = ToyMapDataset(\"../data/classification/toy_example/data.csv\")\n",
    "print(\n",
    "    f\"Number of record(s): {len(toy_map_dataset)}; \"\n",
    "    f\"The 6th and 7th samples in the datset are:\\n{str(toy_map_dataset[6:8])}\"\n",
    ")\n",
    "\n",
    "# So, of course we can iterate through the dataset\n",
    "# for row in toy_map_dataset:\n",
    "#     print(row)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of record(s): 8; The 6th and 7th samples in the datset are:\n   factor   target\n6     0.0        1\n7     3.0        1\n"
     ]
    }
   ]
  },
  {
   "source": [
    "Although this is already an working example, we can not use it directly for model training as the return values are not `torch.tensor`. Thus, we must do some pre-processing before feeding the data into our model/computation graph, and this set of pre-processing operations is best enclosed in the `Dataset` object. So, let's do some slight modification towards the previous example. **Note** that there are lots of ways in terms of actual implementation, as long as ensuring that **The Return Values Are Tensors** (or list of tensors)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyMapDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Besides fetching the raw data.csv file, we convert \n",
    "          the dataset into collection of tensors. Below is \n",
    "          just one implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \n",
    "        self.df = pd.read_csv(path)\n",
    "        \n",
    "        # Store XY in tensors\n",
    "        self.factor = torch.tensor(\n",
    "            self.df.iloc[:, 0].values  # <-- Hard-coded column names\n",
    "            , dtype=torch.float        #   well, not usually a problem since\n",
    "        )                              #   we pair loaders with src data file\n",
    "        self.target = torch.tensor(\n",
    "            self.df.iloc[:, 1].values\n",
    "            , dtype=torch.float\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):  \n",
    "\n",
    "        # Returns pairs factor(s) and target\n",
    "        return self.factor[idx], self.target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([5.0000, 2.5000]) <class 'torch.Tensor'>\ntensor([1., 1.]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Instantiate one toy dataset and see its feature\n",
    "toy_map_dataset = ToyMapDataset(\"../data/classification/toy_example/data.csv\")\n",
    "\n",
    "# Fetch two samples\n",
    "x, y = toy_map_dataset[1:3]\n",
    "print(x, type(x))\n",
    "print(y, type(y))"
   ]
  },
  {
   "source": [
    "To be honest, up to this point, we don't really need the sampler or dataloader since we can simply iterate through the dataset using a for loop anyway. But, it would be much nicer to enclose the details and only expose nice and clean interfaces to make the code much understandable, and the `DataLoader` object is just designed for that purpose. It might be weird that I don't introduce the `Sampler` first because I said that `DataLoader` is composed of `Dataset` and `Sampler`. The reason is that, by default, `DataLoader` will automatically assign us a `Sampler` (actually two, explained later)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## The DataLoader Object\n",
    "\n",
    "The official documentation can be found [here](https://pytorch.org/docs/stable/data.html). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([10.]) tensor([0.])\ntensor([5.]) tensor([1.])\ntensor([2.5000]) tensor([1.])\ntensor([11.]) tensor([0.])\ntensor([15.]) tensor([0.])\ntensor([20.]) tensor([0.])\ntensor([0.]) tensor([1.])\ntensor([3.]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "toy_map_dataloader = DataLoader(toy_map_dataset)\n",
    "for x, y in toy_map_dataloader:\n",
    "    print(x, y)"
   ]
  },
  {
   "source": [
    "## The Sampler Object\n",
    "\n",
    "The official documentation can be found [here](https://pytorch.org/docs/stable/data.html). In a word, samplers are **indices generator**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Sampler"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Batch Sampler"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Custom Data Loader for Training Using the Iris Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataLoader(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Large Dataset: Read by Chunks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyLargeDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "\n",
    "        self.path = path\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        self.df = pd.read_csv(self.path, chunksize=4)\n",
    "        for dfr in self.df:\n",
    "\n",
    "            fct = torch.tensor(dfr.iloc[:, 0].values)\n",
    "            tgt = torch.tensor(dfr.iloc[:, 1].values)\n",
    "            yield fct, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}