{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('th-federated': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c2ea6ed944d6425ee8782cf654adb8ea89a16b622c320e18cab62be437316a17"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from easydict import EasyDict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import (\n",
    "    Dataset\n",
    "    , IterableDataset\n",
    "    , DataLoader\n",
    "    , SubsetRandomSampler\n",
    "    , random_split\n",
    ")"
   ]
  },
  {
   "source": [
    "# Building a Data Pipeline\n",
    "\n",
    "To build a data pipline with `PyTorch`, there are three major components:\n",
    "\n",
    "* `Dataset`\n",
    "* `DataLoader`\n",
    "* `Sampler`\n",
    "\n",
    "In general, the workflow would be: (1) load raw dataset from disk/web to create a torch `Dataset` object, (2) determine a sampling scheme and instantiate `Sampler` object(s), and lastly (3) glue the `Dataset` and `Sampler`, together with other training parameters, to generate a `DataLoader`. During the model training, we load batches of data by iterating through the `DataLoader` we have created. Examples will be given below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## The Dataset Object\n",
    "\n",
    "The official documentation can be found [here](https://pytorch.org/docs/stable/data.html). There are two types of datasets: (1) **Map-style Dataset** and (2) **Iterable-style Dataset**. In a word, map-style datasets act like a **list** or **table** that we can index on. For instance, we can select the 7th sample. On the other hand, the iterable-style datasets work like iterators, meaning that we only define how we to **retrieve next sample/batch**. Both of these two types of datasets can be very useful. I will introduce the map-style dataset as it is more intuitive. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Map-style Dataset\n",
    "\n",
    "To construct a custom map-style dataset, we must implement at least three methods:\n",
    "\n",
    "* `__init__(self, ...)` (obviously)\n",
    "* `__len__(self)`\n",
    "* `__getitem__(self, idx)`\n",
    "\n",
    "Just in case, the third method is actually called through the square bracket operator `[]`. For instance `my_dataset[0]` gives the first item (well, it depends on how you would implement the method, e.g. whether it supports ranged slicing).\n",
    "\n",
    "A simple template is shown below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, path):  # <-- My habit, not necessarily <path> only\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "# Sample creation of a dataset\n",
    "try:\n",
    "    my_dataset = MyDataset(\"~/path/to/your/raw/data.csv\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "source": [
    "#### A Toy Example\n",
    "\n",
    "The toy example consists only 10 samples with two attributes: factor and target, both of which are numerical data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   factor   target\n",
       "0    10.0        0\n",
       "1     5.0        1\n",
       "2     2.5        1\n",
       "3    11.0        0\n",
       "4    15.0        0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>factor</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df_toy = pd.read_csv(\"../data/classification/toy_example/data.csv\")\n",
    "df_toy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "slice(0, 5, None)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   factor   target\n",
       "0    10.0        0\n",
       "1     5.0        1\n",
       "2     2.5        1\n",
       "3    11.0        0\n",
       "4    15.0        0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>factor</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "class ToyMapDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \n",
    "        self.df = pd.read_csv(path)\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.df.iloc[idx, :]\n",
    "\n",
    "t = ToyMapDataset(\"../data/classification/toy_example/data.csv\")\n"
   ]
  },
  {
   "source": [
    "## The Iris Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## The Toxic-Comments Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Large Dataset: Read by Chunks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyLargeDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "\n",
    "        self.path = path\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        self.df = pd.read_csv(self.path, chunksize=4)\n",
    "        for dfr in self.df:\n",
    "\n",
    "            fct = torch.tensor(dfr.iloc[:, 0].values)\n",
    "            tgt = torch.tensor(dfr.iloc[:, 1].values)\n",
    "            yield fct, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ToyLargeDataset(\"../data/classification/toy_example/data.csv\")\n",
    "l = DataLoader(t, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0, 1, 1, 0])\ntensor([0, 0, 1, 1])\ntensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "for x, y in l:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0, 1, 1, 0])\ntensor([0, 0, 1, 1])\ntensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "for x, y in l:\n",
    "    print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}